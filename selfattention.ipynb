{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.52268931 -1.4782581  -0.03439015 -1.95761889 -0.26820739  0.59758285\n",
      "   0.13767827  0.37600382]\n",
      " [-0.62291954  0.43579031 -1.33992322  0.22032349 -0.11711665  0.95108453\n",
      "   0.26710959  1.98511966]\n",
      " [ 0.16387946 -0.047156   -1.5429586   0.07287787  1.46454299  1.12197514\n",
      "  -0.07419645 -0.23087365]\n",
      " [-1.67441113  1.32634428 -1.26751411  1.71596718  0.5725955  -0.08130886\n",
      "  -0.02004459 -0.48953263]]\n",
      "-------------------------------------------\n",
      "[[-0.52343824 -1.73711874  0.12384902  0.05038051 -1.22395884  1.08990418\n",
      "   0.45273098  0.23054131]\n",
      " [-0.79363833 -0.1607422  -0.89809427  0.40309064  0.15168338 -1.50029654\n",
      "  -0.37666763  1.02757665]\n",
      " [ 0.07639509  0.68486991  0.51118305 -0.01769179 -0.64440864 -0.85676097\n",
      "   0.90291828  0.07050913]\n",
      " [ 0.27640205  0.33491821 -1.47982822 -1.51338878 -0.05814828  1.09844285\n",
      "   0.25208242  0.70455145]]\n",
      "-------------------------------------------\n",
      "[[-0.89674785 -0.63452837 -1.86045605 -1.38436558 -0.01571999 -0.76083829\n",
      "  -1.33394303  1.15265836]\n",
      " [-0.24595801  0.83713414  2.11415451 -0.1876077   0.22100072  0.94753697\n",
      "  -0.64032009  0.27390428]\n",
      " [-0.18726216  0.20462088  0.90341253 -0.5786664  -2.10612244 -0.48143045\n",
      "  -0.2611059   0.10779216]\n",
      " [-0.03528443 -1.58555111  0.49561588 -1.84304    -0.15563197  1.00681295\n",
      "   0.40012817 -0.38148453]]\n"
     ]
    }
   ],
   "source": [
    "# creating Query, Key, Value\n",
    "L, d_K, d_V = 4, 8, 8\n",
    "q = np.random.randn(L, d_K)\n",
    "k = np.random.randn(L, d_K)\n",
    "v = np.random.randn(L, d_V) \n",
    "print(q,k,v, sep=\"\\n-------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is selfattention fomula\n",
    "# selfattention = softmax(q*k.T/sqrt(d_K) + M)v\n",
    "# for decreasing the variance of attention\n",
    "scale = np.matmul(q, k.T)/ math.sqrt(d_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking : this is to ensure words don't get context from words generated in the future and decorder need it\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask==0] = -np.infty\n",
    "mask[mask==1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.55233172,        -inf,        -inf,        -inf],\n",
       "       [ 0.41461562,  0.78173775,        -inf,        -inf],\n",
       "       [-0.29974235, -0.13358094, -0.9892779 ,        -inf],\n",
       "       [-0.85187991,  0.94012438, -0.08830778, -0.42864053]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax \n",
    "# softmax formula is e^xi/ sum(e^xj)\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T) / (np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.07051535, -0.36420375, -0.61206494, -1.84951622, -0.72497137,\n",
       "        -0.45821618, -1.65583438,  1.26919379],\n",
       "       [-0.24756664, -0.30683886,  1.92526485, -1.40176069, -0.88397546,\n",
       "         0.91392933, -0.26824001, -0.00522564],\n",
       "       [-0.04212323, -0.28047744,  0.26863182, -0.47876599, -0.42526455,\n",
       "         0.11234896,  0.03159739, -0.0565286 ],\n",
       "       [-0.00504725, -0.22680441,  0.07089514, -0.26363678, -0.0222623 ,\n",
       "         0.14401908,  0.05723614, -0.05456927]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scale + mask)\n",
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.89674785, -0.63452837, -1.86045605, -1.38436558, -0.01571999,\n",
       "        -0.76083829, -1.33394303,  1.15265836],\n",
       "       [-0.24595801,  0.83713414,  2.11415451, -0.1876077 ,  0.22100072,\n",
       "         0.94753697, -0.64032009,  0.27390428],\n",
       "       [-0.18726216,  0.20462088,  0.90341253, -0.5786664 , -2.10612244,\n",
       "        -0.48143045, -0.2611059 ,  0.10779216],\n",
       "       [-0.03528443, -1.58555111,  0.49561588, -1.84304   , -0.15563197,\n",
       "         1.00681295,  0.40012817, -0.38148453]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfattention(query, key, value, mask_val=None):\n",
    "    scale = np.matmul(query * key.T) / math.sqrt(d_K)\n",
    "    if mask_val != None:\n",
    "        mask = np.tril(np.ones((L,L)))\n",
    "        mask[mask==0] = -np.infty\n",
    "        mask[mask==1] = 0\n",
    "        attention = softmax(scale + mask)\n",
    "        new_v = np.matmul(attention, value)\n",
    "        return new_v\n",
    "    \n",
    "    attention = softmax(scale)\n",
    "    new_v = np.matmul(attention, value)\n",
    "    return new_v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
